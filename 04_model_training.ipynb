{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting/exploring Neural Networks and Transfer Learning with PyTorch\n",
        "\n",
        "In this notebook we experiment with Neural Networks using PyTorch, as well we start exploring transfer learning. The following articles were used as reference to build the networks:\n",
        "\n",
        "* https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "* https://www.pluralsight.com/guides/introduction-to-resnet"
      ],
      "metadata": {
        "id": "trCMalnrpOL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting google drive\n",
        "\n",
        "In order to make it easy to read data from Google Drive, it can be mounted as a \"local\" unity. This encapsulates the connection to Google API and reduces the amount of coding that would be needed when interacting with Google API"
      ],
      "metadata": {
        "id": "vfeJU7Z_l-gd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8BatbEMlpJsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "246c1e17-9a16-4a43-b2a7-89fc4867ff19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "kt3hEXtAgE4a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in the previous notebook we load the dataset into a tensor and then augment it"
      ],
      "metadata": {
        "id": "PvLdwfkRmHkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder = \"/content/gdrive/MyDrive/08 CS670 Artificial Intelligence/Term Project\"\n",
        "data_folder = os.path.join(base_folder, \"transformed_images\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(255),\n",
        "    transforms.ToTensor()]\n",
        ")\n",
        "\n",
        "dataset = datasets.ImageFolder(data_folder, transform=transform)"
      ],
      "metadata": {
        "id": "wEXlqWi7gbd-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform =  transforms.Compose([\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()])\n",
        "dataset_augmented = datasets.ImageFolder(data_folder, transform=transform)"
      ],
      "metadata": {
        "id": "sSnv47c6gePM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_dataset = torch.utils.data.ConcatDataset([dataset, dataset_augmented])\n",
        "final_dataset = torch.utils.data.ConcatDataset([dataset])\n",
        "generator1 = torch.Generator().manual_seed(42)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(final_dataset, [0.75, 0.25])"
      ],
      "metadata": {
        "id": "rZME9N4Iggb0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "image, label = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "iuvmeE1Hh3j-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "heLdbEr3j9Qq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Neural Network\n",
        "\n",
        "To experiment with NN we build a simple SoftMax regression with only two layers:\n",
        "\n",
        "* A layer to flat the 3D Tensor (width X height X channels)\n",
        "* Linear regression with two output channels (one for each class of label)\n",
        "\n",
        "The SoftMax itself (getting the label withe the highest probability) is applied after prediction, as the network outputs the probability for the two classes."
      ],
      "metadata": {
        "id": "iDLUQ7UJmg36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftMaxRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # define layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(195075, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Now it only takes a call to the layer to make predictions\n",
        "        y = self.flatten(x)\n",
        "        y = self.linear(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "z-r2mwhgi0v2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we check if GPU is available, instantiate our model, pick our loss functions and define our optimizer. We are using one example from Deep Learning class without worrying about \"the best choice\", as this is just an experiment.\n",
        "\n",
        "**Note**: in Google Colab, do not forget to change the runtime environment to one with GPU"
      ],
      "metadata": {
        "id": "iHo2bfndnURh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# let's instantiate a model\n",
        "model = SoftMaxRegression().to(device)\n",
        "\n",
        "# define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# loss_fn = nn.NLLLoss()\n",
        "\n",
        "# define optimizer\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsMF8FcZi8yQ",
        "outputId": "eae1ab26-4298-45fc-e6ce-f3c9743d02a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we initiallize the weights for our model"
      ],
      "metadata": {
        "id": "i3t-SqrEnpfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, std=0.01)\n",
        "\n",
        "model.apply(init_weights);"
      ],
      "metadata": {
        "id": "0zThrAXgi_88"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here we check how our model is working for one batch of our training set"
      ],
      "metadata": {
        "id": "jjp0u8TPnwov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x_batch, y_batch in train_dataloader:\n",
        "\n",
        "  print(x_batch.shape)\n",
        "  print(y_batch.shape)\n",
        "  print(\"Features:\\n\\t\", x_batch[:1], \"\\nLabels:\\n\\t\", y_batch[:1])\n",
        "\n",
        "  print(nn.Flatten(x_batch))\n",
        "\n",
        "  x_batch = x_batch.to(device)\n",
        "  y_batch = y_batch.to(device)\n",
        "\n",
        "  y_hat = model(x_batch)\n",
        "  print(y_hat[1])\n",
        "\n",
        "  ll = loss_fn(y_hat, y_batch )\n",
        "#   print(ll)\n",
        "\n",
        "  break   # break after first pair, we just want to observe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC5F5tJyjBtH",
        "outputId": "c3333578-f309-4fb9-c418-af7577dfaddb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 255, 255])\n",
            "torch.Size([128])\n",
            "Features:\n",
            "\t tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]]) \n",
            "Labels:\n",
            "\t tensor([1])\n",
            "Flatten(\n",
            "  start_dim=tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "  \n",
            "  \n",
            "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "  \n",
            "  \n",
            "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "  \n",
            "  \n",
            "          ...,\n",
            "  \n",
            "  \n",
            "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "  \n",
            "  \n",
            "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "  \n",
            "  \n",
            "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "  \n",
            "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            ...,\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "            [0., 0., 0.,  ..., 0., 0., 0.]]]]), end_dim=-1\n",
            ")\n",
            "tensor([-0.0668,  1.3127], device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here we built our train step (epoch) and test it"
      ],
      "metadata": {
        "id": "hqoMOBeJn4tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    # Builds function that performs a step in the train loop\n",
        "    def train_step(x, y):\n",
        "        # Sets model to TRAIN mode\n",
        "        model.train()\n",
        "        # Makes predictions\n",
        "        yhat = model(x)\n",
        "        # Computes loss\n",
        "        loss = loss_fn(yhat, y)\n",
        "        # Computes gradients\n",
        "        loss.backward()\n",
        "        # Updates parameters and zeroes gradients\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Returns the loss\n",
        "        return loss.item()\n",
        "\n",
        "    # Returns the function that will be called inside the train loop\n",
        "    return train_step\n",
        "\n",
        "# Creates the train_step function for our model, loss function and optimizer\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "losses = []\n",
        "n_epochs = 10"
      ],
      "metadata": {
        "id": "BVjk0Kazkv1F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we train our model for 5 epochs (iterations)"
      ],
      "metadata": {
        "id": "onhrO5E6oF0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "model.apply(init_weights)   #always good to initialize in the beginning\n",
        "n_epochs = 5\n",
        "losses = []\n",
        "test_losses = []\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(epoch + 1, \"/\", n_epochs)\n",
        "    for x_batch, y_batch in train_dataloader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "\n",
        "    print(\"Train loss:\", np.mean(losses))\n",
        "\n",
        "    # torch no_grad makes sure that the nested-below computations happen without gradients,\n",
        "    # since these are not needed for evaluation\n",
        "    with torch.no_grad():\n",
        "        for x_test, y_test in test_dataloader:\n",
        "            x_test = x_test.to(device)\n",
        "            y_test = y_test.to(device)\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            yhat = model(x_test)\n",
        "            test_loss = loss_fn(yhat, y_test)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "        print(\"Test loss:\", np.mean(test_losses))\n",
        "#print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw4HmsSgk00w",
        "outputId": "8cc13abc-1e20-4316-f93d-8d69698579fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "1 / 5\n",
            "Train loss: 8.616334774277426\n",
            "Test loss: 4.210230523889715\n",
            "2 / 5\n",
            "Train loss: 5.395738652258208\n",
            "Test loss: 2.724452265284278\n",
            "3 / 5\n",
            "Train loss: 3.932867935510597\n",
            "Test loss: 2.0165023948207046\n",
            "4 / 5\n",
            "Train loss: 3.089810702272437\n",
            "Test loss: 1.6321038888259367\n",
            "5 / 5\n",
            "Train loss: 2.542619027152206\n",
            "Test loss: 1.3790089810436421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the accuracy of trained model"
      ],
      "metadata": {
        "id": "QyaAzDWkra0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(net, test_dataloader):\n",
        "\n",
        "    n_samples = 0;\n",
        "    n_correct = 0;\n",
        "    model.eval()\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        trues = y\n",
        "\n",
        "        preds = model(X).argmax(axis=1)\n",
        "\n",
        "\n",
        "        n_samples = n_samples + y.shape[0]\n",
        "        n_correct = n_correct + (trues==preds).sum()\n",
        "#         break\n",
        "\n",
        "    return n_correct/n_samples\n",
        "\n",
        "accuracy(model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31xQ60ZXppLq",
        "outputId": "a1ae801f-686e-421b-c5f6-fbe480e54270"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9325, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even without worrying about \"the right choice\" or performance, our test model was able to achieve an accuracy of 0.9325 (93.25%). It is important to notice that our test loss was always below the train loss, which indicates a good generalization of the model."
      ],
      "metadata": {
        "id": "cEdmH2A1wx38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "We use PyTorch to load one of the several pretrained models available and check its architecture."
      ],
      "metadata": {
        "id": "CacIXA2Grefa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "net = torchvision.models.resnet101(pretrained=True)\n",
        "net = net.cuda() if torch.cuda.is_available() else net\n",
        "net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_ECGd-5t9mx",
        "outputId": "839d9563-2e7d-441b-a809-64684aca2b66"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:01<00:00, 139MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (9): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (10): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (11): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (12): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (13): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (14): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (15): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (16): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (17): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (18): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (19): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (20): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (21): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (22): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}